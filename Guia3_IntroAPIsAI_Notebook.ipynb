{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32b2a27",
   "metadata": {},
   "source": [
    "# Guía de Instalación y Práctica — Sesión 1 (Notebook)\n",
    "Curso: **IA en el Aula — Nivel Avanzado**  \n",
    "Profesor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **22 de octubre de 2025**\n",
    "\n",
    "Este cuaderno guía a los participantes para configurar el entorno, conectarse a una API de modelos de lenguaje y realizar las primeras consultas con **parámetros clave** como `temperature`, `max_tokens`, `top_p`, etc. Se incluyen explicaciones paso a paso y ejercicios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0ddd",
   "metadata": {},
   "source": [
    "## 1) Requisitos previos\n",
    "- Python 3.10 o superior\n",
    "- Cuenta y **clave API** en un proveedor de modelos de lenguaje (ej. OpenAI)\n",
    "- Editor/entorno: VSCode, Jupyter o Colab\n",
    "- Conexión a Internet\n",
    "\n",
    "Si estás en **Colab**, puedes ejecutar el siguiente comando para instalar dependencias. En local, usa tu terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc702d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# (Opcional en local/Colab) Instalar dependencias\n",
    "%pip install --quiet openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5e55a",
   "metadata": {},
   "source": [
    "## 2) Preparar variables de entorno\n",
    "Crea un archivo `.env` en la carpeta del proyecto con:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=tu_clave_aqui\n",
    "```\n",
    "Nunca publiques tu clave. Evita subir `.env` a repositorios públicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80b7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente inicializado. Modelo listo para consultas.\n"
     ]
    }
   ],
   "source": [
    "# 3) Cargar la clave y crear el cliente\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Lee el archivo .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00221859",
   "metadata": {},
   "source": [
    "Si está en Google Colab, debe incluir la llave en los secretos (en el menú de la izquierda), activar la variable y luego inicializar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd38611",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3) Cargar la clave y crear el cliente en google Colab\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      6\u001b[39m client = OpenAI(api_key=userdata.get(\u001b[33m'\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCliente inicializado. Modelo listo para consultas.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# 3) Cargar la clave y crear el cliente en google Colab\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174eb5a6",
   "metadata": {},
   "source": [
    "## 4) Parámetros importantes de la API (explicación breve)\n",
    "- **`model`**: identifica el modelo a consultar (p.ej., `gpt-4o-mini`).\n",
    "- **`messages`**: lista de turnos conversacionales. Usa roles `system` (instrucciones generales), `user` (tu prompt), `assistant` (respuestas previas si las hay).\n",
    "- **`temperature`**: *aleatoriedad* de la salida. Valores bajos (0.0–0.3) hacen respuestas más deterministas; valores altos (0.7–1.0) hacen respuestas más creativas.\n",
    "- **`top_p`**: alternativa a `temperature` basada en muestreo por probabilidad acumulada; usa uno u otro (no ambos a la vez a valores lejanos) para afinar el estilo.\n",
    "- **`max_tokens`**: tope de tokens generados en la respuesta. Si es muy bajo, el texto puede cortarse.\n",
    "- **`stop`**: secuencias de corte; si aparecen, la generación se detiene.\n",
    "- **`frequency_penalty` / `presence_penalty`**: penalizaciones para reducir repeticiones y fomentar aparición de nuevos términos.\n",
    "\n",
    "**Buenas prácticas:**\n",
    "- Controlar `temperature` (0.2–0.7) según la tarea (evaluación → bajo; lluvia de ideas → medio/alto).\n",
    "- Estructurar prompts y, cuando sea útil, **pedir JSON** para facilitar la automatización.\n",
    "- Evitar incluir datos personales o sensibles en prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873e7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial en la educación se refiere al uso de tecnologías avanzadas para personalizar el aprendizaje, adaptando contenido y métodos a las necesidades individuales de los estudiantes. Además, puede facilitar la automatización de tareas administrativas y proporcionar análisis de datos para mejorar la enseñanza y el rendimiento académico.\n"
     ]
    }
   ],
   "source": [
    "# 5) Primera consulta: respuesta libre\n",
    "prompt = \"Explica en dos frases qué es la inteligencia artificial en la educación.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7  # más creativo que 0.2, menos que 1.0\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e281741",
   "metadata": {},
   "source": [
    "### Comentarios sobre `temperature`\n",
    "- A `0.0–0.2`: respuestas casi siempre iguales (útil en **rubricas** o **explicaciones estándar**).\n",
    "- A `0.5–0.8`: más variación léxica/estilística (útil en **generación de materiales** o **brainstorming**).\n",
    "- A `>0.9`: creatividad alta pero riesgo de salidas menos precisas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f22c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"operation\": \"explanation\",\n",
      "  \"input\": \"¿Qué es aprendizaje supervisado?\",\n",
      "  \"output\": \"El aprendizaje supervisado es un tipo de aprendizaje automático en el que un modelo es entrenado utilizando un conjunto de datos etiquetados. Esto significa que cada entrada del conjunto de datos tiene una salida correspondiente conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza comúnmente en tareas como clasificación y regresión.\"\n",
      "}\n",
      "\n",
      "Valid JSON → {'operation': 'explanation', 'input': '¿Qué es aprendizaje supervisado?', 'output': 'El aprendizaje supervisado es un tipo de aprendizaje automático en el que un modelo es entrenado utilizando un conjunto de datos etiquetados. Esto significa que cada entrada del conjunto de datos tiene una salida correspondiente conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza comúnmente en tareas como clasificación y regresión.'}\n"
     ]
    }
   ],
   "source": [
    "# 6) Respuesta estructurada en JSON para automatización\n",
    "import json\n",
    "\n",
    "query = \"¿Qué es aprendizaje supervisado?\"\n",
    "schema_instruction = (\n",
    "    \"Responde en formato JSON con las claves: operation, input, output. \"\n",
    "    \"operation debe ser 'explanation'; input debe repetir la pregunta; output la explicación clara y breve.\"\n",
    ")\n",
    "response_json = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": schema_instruction},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ],\n",
    "    temperature=0.3,       # más determinista para formatos estructurados\n",
    "    max_tokens=300         # suficiente para una explicación breve\n",
    ")\n",
    "text = response_json.choices[0].message.content\n",
    "print(text)\n",
    "\n",
    "# (Opcional) intentar cargar como JSON si el modelo devolvió un objeto válido\n",
    "try:\n",
    "    data = json.loads(text)\n",
    "    print(\"\\nValid JSON →\", data)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\nLa salida no es JSON válido literal. Puedes parsearla manualmente o usar validadores/funciones JSON del proveedor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a1d82",
   "metadata": {},
   "source": [
    "## 7) Ejercicios propuestos\n",
    "1. Cambia `temperature` a 0.1, 0.5 y 0.9 y compara el estilo de las respuestas.\n",
    "2. Pide que el modelo responda **siempre en JSON** usando un `system` que lo exija. Verifica si cumple.\n",
    "3. Crea un prompt de tu área (p.ej., *programación*, *arquitectura*, *matemáticas*) que devuelva un JSON con `operation`, `input`, `steps` (lista) y `output`.\n",
    "4. Limita la longitud con `max_tokens` y observa si corta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eece738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"Explica brevemente el principio de funcionamiento de un árbol de decisión.\",\n",
      "  \"respuesta\": \"Un árbol de decisión es un modelo de predicción que utiliza un enfoque de ramificación para tomar decisiones basadas en características de los datos. Cada nodo interno representa una prueba sobre un atributo, cada rama representa el resultado de esa prueba y cada hoja representa una clase o resultado final, permitiendo clasificar o predecir resultados a partir de las decisiones tomadas en cada nodo.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plantilla reutilizable para el curso\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.3,\n",
    "              max_tokens: int = 100,\n",
    "              system: str | None = None):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(ask_model(\n",
    "    \"Explica brevemente el principio de funcionamiento de un árbol de decisión.\",\n",
    "    temperature=0.4,\n",
    "    system=\"Responde en dos oraciones, tono docente y preciso. Siempre responde JSON con llaves promt y respuesta\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fef7c",
   "metadata": {},
   "source": [
    "## 9) Solución de problemas comunes\n",
    "- **`openai.AuthenticationError` / `401`**: clave inválida o no cargada; revisa tu `.env` y reinicia el kernel.\n",
    "- **`Rate limit`**: excediste el número de solicitudes por minuto; espera unos segundos y reintenta.\n",
    "- **`model_not_found`**: el modelo no existe o no tienes acceso; cambia a uno disponible en tu cuenta.\n",
    "- **Salidas no JSON**: fija `temperature=0.0–0.3`, agrega instrucciones `system` estrictas y/o usa funciones nativas de validación JSON si el proveedor las ofrece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eab4f2",
   "metadata": {},
   "source": [
    "## 10) Próximos pasos\n",
    "En la siguiente sesión se verán **consultas avanzadas**, manejo de contexto y el punto de partida para construir un **asistente con RAG** (recuperación de conocimiento) usando materiales del curso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
